{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Becoming backprop ninja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lecture is more of a workbook style lecture it seems. I am going to first do the exercises myself and then see how Andrej implements it. The first few cells are the same as before where you do your standard imports and then prepare your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('names.txt') as f:\n",
    "    names = f.readlines()\n",
    "    names = [name.strip('\\n') for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate implementation, used by Andrej\n",
    "names = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the vocab\n",
    "tokens = ['.'] + sorted(set(''.join(names)))\n",
    "stoi = {}\n",
    "itos = {}\n",
    "for i, token in enumerate(tokens):\n",
    "    stoi[token] = i\n",
    "    itos[i] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataset(names):\n",
    "    X, Y = [], []\n",
    "    for name in names:\n",
    "        chars = [0] * 3\n",
    "        for x in name + '.':\n",
    "            xi = stoi[x]\n",
    "            X.append(chars)\n",
    "            Y.append(xi)\n",
    "            chars = chars[1:] + [xi]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(names)\n",
    "\n",
    "samples = len(names)\n",
    "\n",
    "train = 0.8\n",
    "val = 0.9\n",
    "test = 1.0\n",
    "\n",
    "Xtr, Ytr = buildDataset(names[:int(train*samples)])\n",
    "Xval, Yval = buildDataset(names[int(train*samples):int(val*samples)])\n",
    "Xtest, Ytest = buildDataset(names[int(val*samples):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias = True):\n",
    "        self.weights = torch.randn((fan_in, fan_out)) / (fan_in ** 0.5)\n",
    "        self.bias = torch.randn((fan_out,)) * 0.1 if bias else None\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weights\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return [self.weights] + ([] if self.bias is None else [self.bias])\n",
    "    \n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11897\n"
     ]
    }
   ],
   "source": [
    "max_iterations = 20000\n",
    "vocab_size = len(stoi)\n",
    "context_length = 3\n",
    "embedding_dim = 10\n",
    "hidden_dim = 200\n",
    "minibatch_size = 32\n",
    "lr = 0.1\n",
    "\n",
    "C = torch.randn((vocab_size, embedding_dim))\n",
    "layers = [\n",
    "    Linear(embedding_dim*context_length, hidden_dim), Tanh(),\n",
    "    Linear(                  hidden_dim, vocab_size)\n",
    "]\n",
    "\n",
    "parameters = [C] + [p for layer in layers if isinstance(layer, Linear) for p in layer.parameters()]\n",
    "for p in parameters:\n",
    "    p.requires_grad_()\n",
    "\n",
    "print(sum([p.nelement() for p in parameters]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/20000 loss = 2.6045684814453125\n",
      "2000/20000 loss = 2.095168352127075\n",
      "4000/20000 loss = 1.7923805713653564\n",
      "6000/20000 loss = 2.142599582672119\n",
      "8000/20000 loss = 1.9210169315338135\n",
      "10000/20000 loss = 2.3190653324127197\n",
      "12000/20000 loss = 1.9645098447799683\n",
      "14000/20000 loss = 2.592478036880493\n",
      "16000/20000 loss = 2.202781915664673\n",
      "18000/20000 loss = 2.5616817474365234\n",
      "20000/20000 loss = 2.426074266433716\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_iterations + 1):\n",
    "    # minibatch\n",
    "    minibatch = torch.randint(Xtr.shape[0], (minibatch_size,))\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xtr[minibatch]]\n",
    "    embcat = emb.view(-1, embedding_dim * context_length)\n",
    "    x = embcat\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, Ytr[minibatch])\n",
    "    \n",
    "    # calculate gradient\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update values\n",
    "    for p in parameters:\n",
    "        p.data -= lr * p.grad\n",
    "    \n",
    "    # print progress\n",
    "    if step % 2000 == 0:\n",
    "        print(f'{step}/{max_iterations} loss = {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we were just recreating what we did in the previous lectures. Now we \"chunkate\" the code and write it in a way where we don't use a lot of internal functions of PyTorch in order to implement backprop on our own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n"
     ]
    }
   ],
   "source": [
    "max_iterations = 20000\n",
    "vocab_size = len(stoi)\n",
    "context_length = 3\n",
    "embedding_dim = 10\n",
    "hidden_dim = 200\n",
    "minibatch_size = 32\n",
    "lr = 0.1\n",
    "epsilon = 1e-5\n",
    "\n",
    "C = torch.randn((vocab_size, embedding_dim))\n",
    "# Linear layer 1\n",
    "W1 = torch.randn((context_length * embedding_dim, hidden_dim)) * 0.1\n",
    "b1 = torch.randn((hidden_dim,)) * 0.01\n",
    "# Batch norm tensors\n",
    "gamma = torch.ones((hidden_dim,))\n",
    "beta = torch.zeros((hidden_dim,))\n",
    "# Linear layer 2\n",
    "W2 = torch.randn((hidden_dim, vocab_size)) * 0.1\n",
    "b2 = torch.randn((vocab_size,)) * 0.01\n",
    "\n",
    "parameters = [C, W1, b1, gamma, beta, W2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad_()\n",
    "\n",
    "print(sum([p.nelement() for p in parameters]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/20000 loss = 3.775161027908325\n",
      "2000/20000 loss = 2.462073802947998\n",
      "4000/20000 loss = 2.5500526428222656\n",
      "6000/20000 loss = 2.0544486045837402\n",
      "8000/20000 loss = 2.1829376220703125\n",
      "10000/20000 loss = 2.486475706100464\n",
      "12000/20000 loss = 2.3895978927612305\n",
      "14000/20000 loss = 2.130673408508301\n",
      "16000/20000 loss = 2.0279457569122314\n",
      "18000/20000 loss = 2.4456534385681152\n",
      "20000/20000 loss = 2.4373044967651367\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_iterations + 1):\n",
    "    # minibatch\n",
    "    minibatch = torch.randint(Xtr.shape[0], (minibatch_size,))\n",
    "\n",
    "    # ----- forward pass -----\n",
    "    # embedding and concatenating\n",
    "    emb = C[Xtr[minibatch]]\n",
    "    embcat = emb.view(-1, embedding_dim * context_length)\n",
    "    # Linear layer 1\n",
    "    h1preact = embcat @ W1 + b1\n",
    "    # Batch Norm\n",
    "    h1mean = h1preact.mean(0, keepdims=True)\n",
    "    h1var = h1preact.var(0, keepdims=True)\n",
    "    h1shifted = h1preact - h1mean\n",
    "    h1scalefactor = torch.sqrt(h1var + epsilon)\n",
    "    xhat = h1shifted / h1scalefactor\n",
    "    yhat = gamma * xhat + beta\n",
    "    # Activation\n",
    "    h1 = torch.tanh(yhat)\n",
    "    # Linear layer 2 \n",
    "    h2 = h1 @ W2 + b2\n",
    "    # Calculate the loss function\n",
    "    counts = h2.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    logprobs = -probs.log()\n",
    "    predprob = logprobs[torch.arange(minibatch_size), Ytr[minibatch]]\n",
    "    loss = predprob.mean()\n",
    "    \n",
    "    # calculate gradient\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update values\n",
    "    for p in parameters:\n",
    "        p.data -= lr * p.grad\n",
    "    \n",
    "    # print progress\n",
    "    if step % 2000 == 0:\n",
    "        print(f'{step}/{max_iterations} loss = {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have something that works as intended. This is good. Now we will just import the code that Andrej wrote so that we are in unision with the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182479, 3]) torch.Size([182479])\n",
      "torch.Size([22760, 3]) torch.Size([22760])\n",
      "torch.Size([22907, 3]) torch.Size([22907])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(names):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in names:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(names)\n",
    "n1 = int(0.8*len(names))\n",
    "n2 = int(0.9*len(names))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(names[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(names[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(names[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad, atol=1e-7)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.4203, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "block_size = 3\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())\n",
    "\n",
    "# backward pass\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "dlogits = None # TODO. my solution is 3 lines\n",
    "# -----------------\n",
    "\n",
    "#cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_sum_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 1]), torch.Size([32, 27]))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, logit_maxes.shape, norm_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-08\n",
      "h               | exact: False | approximate: True  | maxdiff: 1.1175870895385742e-08\n",
      "W2              | exact: False | approximate: True  | maxdiff: 2.60770320892334e-08\n",
      "b2              | exact: False | approximate: True  | maxdiff: 2.3283064365386963e-08\n",
      "hpreact         | exact: False | approximate: True  | maxdiff: 7.916241884231567e-09\n",
      "bngain          | exact: False | approximate: True  | maxdiff: 6.51925802230835e-09\n",
      "bnbias          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-08\n",
      "bnraw           | exact: False | approximate: True  | maxdiff: 9.080395102500916e-09\n",
      "bnvar_inv       | exact: False | approximate: True  | maxdiff: 1.210719347000122e-08\n",
      "bnvar           | exact: False | approximate: True  | maxdiff: 1.6298145055770874e-09\n",
      "bndiff2         | exact: False | approximate: True  | maxdiff: 5.820766091346741e-11\n",
      "bndiff          | exact: False | approximate: True  | maxdiff: 6.6356733441352844e-09\n",
      "bnmeani         | exact: False | approximate: True  | maxdiff: 1.6763806343078613e-08\n",
      "hprebn          | exact: False | approximate: True  | maxdiff: 6.28642737865448e-09\n",
      "embcat          | exact: False | approximate: True  | maxdiff: 1.234002411365509e-08\n",
      "W1              | exact: False | approximate: True  | maxdiff: 2.9685907065868378e-08\n",
      "b1              | exact: False | approximate: True  | maxdiff: 3.841705620288849e-09\n",
      "emb             | exact: False | approximate: True  | maxdiff: 1.234002411365509e-08\n",
      "C               | exact: False | approximate: True  | maxdiff: 2.0489096641540527e-08\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "# -----------------\n",
    "\n",
    "# Older implementations will be in comments in case the implementation needs to be changed\n",
    "\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "# dlogprobs = torch.zeros((logprobs.shape))\n",
    "dlogprobs[range(batch_size), Yb] = - 1/batch_size\n",
    "# for sample, y in zip(range(batch_size), Yb):\n",
    "#    dlogprobs[sample, y] = - 1/batch_size\n",
    "# The for loop is not needed in torch\n",
    "\n",
    "dprobs = dlogprobs * (1/probs)\n",
    "# dprobs = torch.zeros_like(probs)\n",
    "# dprobs[range(batch_size), Yb] = dlogprobs[range(batch_size), Yb] * (1/probs[range(batch_size), Yb])\n",
    "\n",
    "dcounts_sum_inv = (dprobs * counts).sum(1, keepdims = True)\n",
    "\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts_sum = -1 / (torch.square(counts_sum)) * dcounts_sum_inv\n",
    "dcounts += dcounts_sum\n",
    "\n",
    "dnorm_logits = norm_logits.exp() * dcounts\n",
    "dlogit_maxes = -dnorm_logits.sum(1, keepdims=True)\n",
    "\n",
    "dlogits = dnorm_logits.clone()\n",
    "dlogits -= dlogit_maxes\n",
    "\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "\n",
    "dhpreact = dh * (torch.ones_like(h) - (h)**2)\n",
    "dbngain = (dhpreact * bnraw).sum(0, keepdims=True)\n",
    "dbnbias = dhpreact.sum(0, keepdims = True)\n",
    "dbnraw = dhpreact * bngain\n",
    "\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdims = True)\n",
    "\n",
    "dbnvar = dbnvar_inv * ((-(bnvar + epsilon)**-1.5)/2)\n",
    "\n",
    "dbndiff2 = 1/(n-1) * torch.ones_like(bndiff2) * dbnvar\n",
    "\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbndiff += 2 * bndiff * dbndiff2\n",
    "\n",
    "dbnmeani = -dbndiff.sum(0, keepdims = True)\n",
    "dhprebn = dbndiff.clone()\n",
    "dhprebn += 1/n * torch.ones_like(hprebn) * dbnmeani\n",
    "\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0, keepdims=True)\n",
    "\n",
    "demb = dembcat.view(emb.shape)\n",
    "dC = torch.zeros_like(C)\n",
    "for i in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[i,j]\n",
    "        dC[ix] += demb[i,j]\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14d758760>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow((W2.grad - dW2)>1e-7, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14d8fdfd0>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow((embcat.grad - dembcat)>1e-7, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([27, 10]), torch.Size([32, 3]), torch.Size([32, 3, 10]))"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.shape, Xb.shape, emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4203014373779297 diff: 2.384185791015625e-07\n",
      "logits          | exact: False | approximate: True  | maxdiff: 6.51925802230835e-09\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())\n",
    "\n",
    "# backward pass\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "dlogits = None # TODO. my solution is 3 lines\n",
    "# -----------------\n",
    "\n",
    "dlogits = 1/n * F.softmax(logits, dim=1)\n",
    "dlogits[range(len(Yb)),Yb] -= 1/n\n",
    "\n",
    "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpreact.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to play around a bit to understand Bessel's correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0241), tensor(1.0088))"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = 10000\n",
    "samples = torch.randn((num_samples,))\n",
    "samples.mean(), samples.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "biasedvars = []\n",
    "unbiasedvars = []\n",
    "for _ in range(10000):\n",
    "    smallSampleix = torch.randint(num_samples, (64,))\n",
    "    smallSample = samples[smallSampleix]\n",
    "    biasedvars.append(smallSample.var(unbiased=False))\n",
    "    unbiasedvars.append(smallSample.var(unbiased=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9921)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(biasedvars)/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0078)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(unbiasedvars)/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(unbiasedvars,bins=40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAT0ElEQVR4nO3df5Bd5X3f8ffHyPhXHIRho1ElUZFYtuNxa4K3rlx7Mg6qU5AzFp1iipsamdFUnZa6bpxpUfNH6LT9A2baEDNx6ajGtcgktimxIzWhdhkBpW0ixoshgCGO1xgsqQKtMSiNGddR/O0f9wEu8mrvXe3de1eH92tmZ895znP2fq+k+9Gzzz33OakqJEnd8opJFyBJGj3DXZI6yHCXpA4y3CWpgwx3SeqgVZMuAODcc8+tjRs3TroMSTqt3Hfffd+pqqn5jq2IcN+4cSMzMzOTLkOSTitJnjjZMadlJKmDDHdJ6iDDXZI6yHCXpA4y3CWpgwx3Seogw12SOshwl6QOMtwlqYNWxCdU1Q0bd/3+gscfv+79Y6pEkiN3SeogR+4am4VG9o7qpdFy5C5JHTRUuCf5pSRfS/Jwks8meXWS85Pcm2Q2yeeTnNn6vqrtz7bjG5f1GUiSfsTAcE+yDvinwHRVvQ04A7gCuB64oareCDwD7Gin7ACeae03tH6SpDEadlpmFfCaJKuA1wJHgIuA29rxPcClbXtb26cd35IkI6lWkjSUgeFeVYeBfwd8m16oHwPuA56tquOt2yFgXdteBxxs5x5v/c858ecm2ZlkJsnM3NzcUp+HJKnPMNMyZ9MbjZ8P/CXgdcDFS33gqtpdVdNVNT01Ne9doiRJp2iYaZm/CXyrquaq6s+BLwDvBla3aRqA9cDhtn0Y2ADQjp8FPD3SqiVJCxom3L8NbE7y2jZ3vgV4BLgLuKz12Q7sbdv72j7t+J1VVaMrWZI0yDBz7vfSe2P0q8BD7ZzdwDXAx5PM0ptTv7mdcjNwTmv/OLBrGeqWJC1gqE+oVtW1wLUnND8GvHOevt8HPrj00iRJp8rlB7QogxYHk7QyuPyAJHWQ4S5JHWS4S1IHGe6S1EGGuyR1kOEuSR1kuEtSBxnuktRBfohJp4VBH57yHqzSSzlyl6QOMtwlqYMMd0nqIMNdkjrIcJekDjLcJamDBl4KmeTNwOf7mn4S+FXglta+EXgcuLyqnmm34vsEsBV4DvhIVX11tGWra1wnXhqtYW6z9/WquqCqLgDeQS+wv0jv9nn7q2oTsJ8Xb6d3CbCpfe0EblqGuiVJC1jstMwW4JtV9QSwDdjT2vcAl7btbcAt1XMAWJ1k7SiKlSQNZ7HhfgXw2ba9pqqOtO0ngTVtex1wsO+cQ63tJZLsTDKTZGZubm6RZUiSFjJ0uCc5E/gA8F9OPFZVBdRiHriqdlfVdFVNT01NLeZUSdIAi1lb5hLgq1X1VNt/KsnaqjrSpl2OtvbDwIa+89a3Np0GfGNT6obFTMt8iBenZAD2Advb9nZgb1/7lenZDBzrm76RJI3BUCP3JK8D3gf8w77m64Bbk+wAngAub+2307sMcpbelTVXjaxaSdJQhgr3qvoecM4JbU/Tu3rmxL4FXD2S6iRJp8RPqEpSBxnuktRBhrskdZDhLkkdZLhLUgcZ7pLUQYa7JHWQ4S5JHWS4S1IHGe6S1EGGuyR10GKW/JVWrIWWKn78uvePsRJpZXDkLkkdZLhLUgcZ7pLUQYa7JHXQUOGeZHWS25L8cZJHk7wryRuS3JHkG+372a1vktyYZDbJg0kuXN6nIEk60bAj908AX6qqtwBvBx4FdgH7q2oTsL/tQ+9G2pva107gppFWLEkaaOClkEnOAn4W+AhAVf0A+EGSbcB7W7c9wN3ANcA24JZ2u70DbdS/1ptkrwwLXTIoqTuGGbmfD8wB/znJ/Uk+1W6YvaYvsJ8E1rTtdcDBvvMPtbaXSLIzyUySmbm5uVN/BpKkHzFMuK8CLgRuqqqfAb7Hi1MwwAs3xa7FPHBV7a6q6aqanpqaWsypkqQBhgn3Q8Chqrq37d9GL+yfSrIWoH0/2o4fBjb0nb++tUmSxmRguFfVk8DBJG9uTVuAR4B9wPbWth3Y27b3AVe2q2Y2A8ecb5ek8Rp2bZmPAr+V5EzgMeAqev8x3JpkB/AEcHnrezuwFZgFnmt9JUljNFS4V9UDwPQ8h7bM07eAq5dWliRpKfyEqiR1kOEuSR1kuEtSBxnuktRBhrskdZDhLkkdZLhLUgd5g2x13qCVML2BtrrIkbskdZDhLkkdZLhLUgcZ7pLUQYa7JHWQ4S5JHWS4S1IHGe6S1EFDhXuSx5M8lOSBJDOt7Q1J7kjyjfb97NaeJDcmmU3yYJILl/MJSJJ+1GJG7j9XVRdU1fN3ZNoF7K+qTcD+tg9wCbCpfe0EbhpVsZKk4SxlWmYbsKdt7wEu7Wu/pXoOAKuTrF3C40iSFmnYcC/gvye5L8nO1ramqo607SeBNW17HXCw79xDre0lkuxMMpNkZm5u7hRKlySdzLALh72nqg4n+QngjiR/3H+wqipJLeaBq2o3sBtgenp6UedqYYMWypLUfUON3KvqcPt+FPgi8E7gqeenW9r3o637YWBD3+nrW5skaUwGhnuS1yV5/fPbwM8DDwP7gO2t23Zgb9veB1zZrprZDBzrm76RJI3BMNMya4AvJnm+/29X1ZeSfAW4NckO4Ang8tb/dmArMAs8B1w18qolSQsaGO5V9Rjw9nnanwa2zNNewNUjqU6SdEq8E5Ne9hZ6A9q7NOl05fIDktRBhrskdZDhLkkdZLhLUgcZ7pLUQYa7JHWQ4S5JHWS4S1IHGe6S1EGGuyR1kOEuSR1kuEtSBxnuktRBhrskdZDhLkkdNHS4Jzkjyf1Jfq/tn5/k3iSzST6f5MzW/qq2P9uOb1ym2iVJJ7GYkfvHgEf79q8HbqiqNwLPADta+w7gmdZ+Q+snSRqjocI9yXrg/cCn2n6Ai4DbWpc9wKVte1vbpx3f0vpLksZk2JH7rwP/Avhh2z8HeLaqjrf9Q8C6tr0OOAjQjh9r/V8iyc4kM0lm5ubmTq16SdK8BoZ7kl8AjlbVfaN84KraXVXTVTU9NTU1yh8tSS97w9wg+93AB5JsBV4N/DjwCWB1klVtdL4eONz6HwY2AIeSrALOAp4eeeWSpJMaOHKvqn9ZVeuraiNwBXBnVf0icBdwWeu2Hdjbtve1fdrxO6uqRlq1JGlBS7nO/Rrg40lm6c2p39zabwbOae0fB3YtrURJ0mINMy3zgqq6G7i7bT8GvHOePt8HPjiC2qSJ27jr9xc8/vh17x9TJdLi+AlVSeogw12SOmhR0zJaGQZNFUiSI3dJ6iDDXZI6yHCXpA4y3CWpgwx3Seogw12SOshwl6QOMtwlqYMMd0nqIMNdkjrIcJekDjLcJamDDHdJ6qCBq0ImeTVwD/Cq1v+2qro2yfnA5+jdhek+4MNV9YMkrwJuAd5B796pf7eqHl+m+qWJ8mYeWqmGGbn/P+Ciqno7cAFwcZLNwPXADVX1RuAZYEfrvwN4prXf0PpJksZomBtkV1X9Wdt9Zfsq4CLgtta+B7i0bW9r+7TjW5JkVAVLkgYb6mYdSc6gN/XyRuCTwDeBZ6vqeOtyCFjXttcBBwGq6niSY/Smbr5zws/cCewEOO+885b2LDrIG3JIWoqh3lCtqr+oqguA9fRuiv2WpT5wVe2uqumqmp6amlrqj5Mk9VnU1TJV9SxwF/AuYHWS50f+64HDbfswsAGgHT+L3hurkqQxGRjuSaaSrG7brwHeBzxKL+Qva922A3vb9r62Tzt+Z1XVCGuWJA0wzJz7WmBPm3d/BXBrVf1ekkeAzyX5t8D9wM2t/83AbyaZBb4LXLEMdUuSFjAw3KvqQeBn5ml/jN78+4nt3wc+OJLqJEmnxE+oSlIHGe6S1EGGuyR1kOEuSR1kuEtSBxnuktRBhrskdZDhLkkdZLhLUgcZ7pLUQYa7JHXQUDfrkHRqFrrpivdX1XJy5C5JHWS4S1IHGe6S1EGGuyR10DC32duQ5K4kjyT5WpKPtfY3JLkjyTfa97Nbe5LcmGQ2yYNJLlzuJyFJeqlhRu7HgV+uqrcCm4Grk7wV2AXsr6pNwP62D3AJsKl97QRuGnnVkqQFDQz3qjpSVV9t2/+X3s2x1wHbgD2t2x7g0ra9Dbileg4Aq5OsHXXhkqSTW9Sce5KN9O6nei+wpqqOtENPAmva9jrgYN9ph1rbiT9rZ5KZJDNzc3OLrVuStIChwz3JjwG/A/yzqvrT/mNVVUAt5oGrandVTVfV9NTU1GJOlSQNMFS4J3klvWD/rar6Qmt+6vnplvb9aGs/DGzoO319a5MkjckwV8sEuBl4tKp+re/QPmB7294O7O1rv7JdNbMZONY3fSNJGoNh1pZ5N/Bh4KEkD7S2XwGuA25NsgN4Ari8Hbsd2ArMAs8BV42yYKkrFlp3Blx7RkszMNyr6n8BOcnhLfP0L+DqJdbVeYNe2JK0FH5CVZI6yHCXpA4y3CWpgwx3Seogw12SOshwl6QOMtwlqYMMd0nqIMNdkjpomOUHdAr8BKqkSXLkLkkdZLhLUgcZ7pLUQYa7JHWQb6hKK5TrvWspHLlLUgcNc5u9Tyc5muThvrY3JLkjyTfa97Nbe5LcmGQ2yYNJLlzO4iVJ8xtm5P4Z4OIT2nYB+6tqE7C/7QNcAmxqXzuBm0ZTpiRpMQaGe1XdA3z3hOZtwJ62vQe4tK/9luo5AKxOsnZEtUqShnSqc+5rqupI234SWNO21wEH+/odam0/IsnOJDNJZubm5k6xDEnSfJb8hmq7IXadwnm7q2q6qqanpqaWWoYkqc+pXgr5VJK1VXWkTbscbe2HgQ19/da3NkkjttClkl4mqVMdue8Dtrft7cDevvYr21Uzm4FjfdM3kqQxGThyT/JZ4L3AuUkOAdcC1wG3JtkBPAFc3rrfDmwFZoHngKuWoWZJA/gBKA0M96r60EkObZmnbwFXL7UoSdLSuPzAErhmu6SVyuUHJKmDDHdJ6iDDXZI6yHCXpA4y3CWpgwx3SeogL4WUXoZcuqD7DPcFeB27pNOV4S7pJVy6oBucc5ekDjLcJamDDHdJ6qCX/Zy7b5pK6qKXfbhLWhzfcD09GO6SRspr6FeGZZlzT3Jxkq8nmU2yazkeQ5J0ciMfuSc5A/gk8D7gEPCVJPuq6pFRP9YwnFOX9HK0HNMy7wRmq+oxgCSfA7YByxLuhrd0+pjk6/XlNiW0HOG+DjjYt38I+OsndkqyE9jZdv8sydeBc4HvLENNy8FaR+90qROsdTksa525fqQ/bqX8mf7lkx2Y2BuqVbUb2N3flmSmqqYnVNKiWOvonS51grUuh9OlTjg9al2ON1QPAxv69te3NknSmCxHuH8F2JTk/CRnAlcA+5bhcSRJJzHyaZmqOp7knwBfBs4APl1VXxvy9N2Du6wY1jp6p0udYK3L4XSpE06DWlNVk65BkjRiLhwmSR1kuEtSB00k3IddniDJ30lSSSZ2ydEwtSa5PMkjSb6W5LfHXWOrYcE6k5yX5K4k9yd5MMnWCdX56SRHkzx8kuNJcmN7Hg8muXDcNfbVMqjWX2w1PpTkD5K8fdw19tWyYK19/f5akuNJLhtXbSc8/sA6k7w3yQPt9fQ/xlnfCXUM+vs/K8l/TfJHrdarxl3jgqpqrF/03mT9JvCTwJnAHwFvnaff64F7gAPA9LjrHLZWYBNwP3B22/+JFVrnbuAfte23Ao9P6M/0Z4ELgYdPcnwr8N+AAJuBeydR55C1/o2+v/dLVnKtff9O7gRuBy5biXUCq+l9mv28tj/219Miav0V4Pq2PQV8FzhzUvWe+DWJkfsLyxNU1Q+A55cnONG/Aa4Hvj/O4k4wTK3/APhkVT0DUFVHx1wjDFdnAT/ets8C/s8Y63uxiKp76L0ITmYbcEv1HABWJ1k7nupealCtVfUHz/+90xuErB9LYfPXMujPFeCjwO8Ak/g3CgxV598DvlBV3279V3KtBbw+SYAfa32Pj6O2YUwi3OdbnmBdf4f2q/iGqpr0wjEDawXeBLwpyf9OciDJxWOr7kXD1PmvgL+f5BC9kdtHx1Paog3zXFaiHfR+41iRkqwD/jZw06RrGeBNwNlJ7k5yX5IrJ13QAn4D+Gl6A6WHgI9V1Q8nW9KLVtx67kleAfwa8JEJlzKsVfSmZt5Lb+R2T5K/UlXPTrKoeXwI+ExV/fsk7wJ+M8nbVtI/xtNVkp+jF+7vmXQtC/h14Jqq+mFvoLlirQLeAWwBXgP8YZIDVfUnky1rXn8LeAC4CPgp4I4k/7Oq/nSiVTWTGLkPWp7g9cDbgLuTPE5v3nXfhN5UHWYphUPAvqr686r6FvAn9MJ+nIapcwdwK0BV/SHwanqLH600p9XyFUn+KvApYFtVPT3pehYwDXyuvaYuA/5DkksnWtH8DgFfrqrvVdV36L3vNrE3qge4it4UUlXVLPAt4C0TrukFkwj3BZcnqKpjVXVuVW2sqo305jI/UFUzK63W5nfpjdpJci69XysfG2ONMFyd36Y3GiLJT9ML97mxVjmcfcCV7aqZzcCxqjoy6aLmk+Q84AvAh1foyPIFVXV+32vqNuAfV9XvTraqee0F3pNkVZLX0ltR9tEJ13Qy/a+pNcCbGf9r/6TGPi1TJ1meIMm/BmaqasWsQzNkrV8Gfj7JI8BfAP983CO4Iev8ZeA/Jfklem8EfaTa2/zjlOSz9P4zPLfN/18LvLI9j/9I7/2ArcAs8By90dFEDFHrrwLn0BsFAxyvCa0UOEStK8KgOqvq0SRfAh4Efgh8qqoWvLxzUrXSu+jjM0keond11zXtt40VweUHJKmD/ISqJHWQ4S5JHWS4S1IHGe6S1EGGuyR1kOEuSR1kuEtSB/1/2c+JOYXtnlsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(biasedvars,bins=40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([32, 64]))"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hprebn.shape, hpreact.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnbias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.1213e-04, -3.8987e-05, -2.8595e-03,  ...,  1.7372e-03,\n",
       "         -6.0090e-04, -1.4034e-04],\n",
       "        [-6.1274e-04,  9.5163e-04,  1.4944e-03,  ...,  1.0453e-03,\n",
       "         -4.9984e-03,  5.1599e-04],\n",
       "        [ 4.7964e-04,  2.1950e-03,  1.2118e-03,  ..., -9.3275e-04,\n",
       "         -2.1857e-03,  1.5204e-03],\n",
       "        ...,\n",
       "        [-4.3875e-04,  3.1108e-04,  2.8374e-03,  ..., -1.6436e-03,\n",
       "          6.5802e-04,  1.1402e-03],\n",
       "        [ 6.6444e-04,  5.3180e-03,  2.6926e-04,  ...,  2.7807e-04,\n",
       "          9.5291e-05, -3.8349e-05],\n",
       "        [ 4.2304e-03, -2.7769e-04,  1.4421e-03,  ..., -1.4400e-03,\n",
       "          5.7057e-04, -4.0913e-04]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dhpreact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
