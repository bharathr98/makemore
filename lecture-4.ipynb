{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Becoming backprop ninja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lecture is more of a workbook style lecture it seems. I am going to first do the exercises myself and then see how Andrej implements it. The first few cells are the same as before where you do your standard imports and then prepare your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('names.txt') as f:\n",
    "    names = f.readlines()\n",
    "    names = [name.strip('\\n') for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate implementation, used by Andrej\n",
    "names = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the vocab\n",
    "tokens = ['.'] + sorted(set(''.join(names)))\n",
    "stoi = {}\n",
    "itos = {}\n",
    "for i, token in enumerate(tokens):\n",
    "    stoi[token] = i\n",
    "    itos[i] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataset(names):\n",
    "    X, Y = [], []\n",
    "    for name in names:\n",
    "        chars = [0] * 3\n",
    "        for x in name + '.':\n",
    "            xi = stoi[x]\n",
    "            X.append(chars)\n",
    "            Y.append(xi)\n",
    "            chars = chars[1:] + [xi]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(names)\n",
    "\n",
    "samples = len(names)\n",
    "\n",
    "train = 0.8\n",
    "val = 0.9\n",
    "test = 1.0\n",
    "\n",
    "Xtr, Ytr = buildDataset(names[:int(train*samples)])\n",
    "Xval, Yval = buildDataset(names[int(train*samples):int(val*samples)])\n",
    "Xtest, Ytest = buildDataset(names[int(val*samples):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias = True):\n",
    "        self.weights = torch.randn((fan_in, fan_out)) / (fan_in ** 0.5)\n",
    "        self.bias = torch.randn((fan_out,)) * 0.1 if bias else None\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weights\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return [self.weights] + ([] if self.bias is None else [self.bias])\n",
    "    \n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11897\n"
     ]
    }
   ],
   "source": [
    "max_iterations = 20000\n",
    "vocab_size = len(stoi)\n",
    "context_length = 3\n",
    "embedding_dim = 10\n",
    "hidden_dim = 200\n",
    "minibatch_size = 32\n",
    "lr = 0.1\n",
    "\n",
    "C = torch.randn((vocab_size, embedding_dim))\n",
    "layers = [\n",
    "    Linear(embedding_dim*context_length, hidden_dim), Tanh(),\n",
    "    Linear(                  hidden_dim, vocab_size)\n",
    "]\n",
    "\n",
    "parameters = [C] + [p for layer in layers if isinstance(layer, Linear) for p in layer.parameters()]\n",
    "for p in parameters:\n",
    "    p.requires_grad_()\n",
    "\n",
    "print(sum([p.nelement() for p in parameters]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/20000 loss = 2.6045684814453125\n",
      "2000/20000 loss = 2.095168352127075\n",
      "4000/20000 loss = 1.7923805713653564\n",
      "6000/20000 loss = 2.142599582672119\n",
      "8000/20000 loss = 1.9210169315338135\n",
      "10000/20000 loss = 2.3190653324127197\n",
      "12000/20000 loss = 1.9645098447799683\n",
      "14000/20000 loss = 2.592478036880493\n",
      "16000/20000 loss = 2.202781915664673\n",
      "18000/20000 loss = 2.5616817474365234\n",
      "20000/20000 loss = 2.426074266433716\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_iterations + 1):\n",
    "    # minibatch\n",
    "    minibatch = torch.randint(Xtr.shape[0], (minibatch_size,))\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xtr[minibatch]]\n",
    "    embcat = emb.view(-1, embedding_dim * context_length)\n",
    "    x = embcat\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, Ytr[minibatch])\n",
    "    \n",
    "    # calculate gradient\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update values\n",
    "    for p in parameters:\n",
    "        p.data -= lr * p.grad\n",
    "    \n",
    "    # print progress\n",
    "    if step % 2000 == 0:\n",
    "        print(f'{step}/{max_iterations} loss = {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we were just recreating what we did in the previous lectures. Now we \"chunkate\" the code and write it in a way where we don't use a lot of internal functions of PyTorch in order to implement backprop on our own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n"
     ]
    }
   ],
   "source": [
    "max_iterations = 20000\n",
    "vocab_size = len(stoi)\n",
    "context_length = 3\n",
    "embedding_dim = 10\n",
    "hidden_dim = 200\n",
    "minibatch_size = 32\n",
    "lr = 0.1\n",
    "epsilon = 1e-5\n",
    "\n",
    "C = torch.randn((vocab_size, embedding_dim))\n",
    "# Linear layer 1\n",
    "W1 = torch.randn((context_length * embedding_dim, hidden_dim)) * 0.1\n",
    "b1 = torch.randn((hidden_dim,)) * 0.01\n",
    "# Batch norm tensors\n",
    "gamma = torch.ones((hidden_dim,))\n",
    "beta = torch.zeros((hidden_dim,))\n",
    "# Linear layer 2\n",
    "W2 = torch.randn((hidden_dim, vocab_size)) * 0.1\n",
    "b2 = torch.randn((vocab_size,)) * 0.01\n",
    "\n",
    "parameters = [C, W1, b1, gamma, beta, W2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad_()\n",
    "\n",
    "print(sum([p.nelement() for p in parameters]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/20000 loss = 3.775161027908325\n",
      "2000/20000 loss = 2.462073802947998\n",
      "4000/20000 loss = 2.5500526428222656\n",
      "6000/20000 loss = 2.0544486045837402\n",
      "8000/20000 loss = 2.1829376220703125\n",
      "10000/20000 loss = 2.486475706100464\n",
      "12000/20000 loss = 2.3895978927612305\n",
      "14000/20000 loss = 2.130673408508301\n",
      "16000/20000 loss = 2.0279457569122314\n",
      "18000/20000 loss = 2.4456534385681152\n",
      "20000/20000 loss = 2.4373044967651367\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_iterations + 1):\n",
    "    # minibatch\n",
    "    minibatch = torch.randint(Xtr.shape[0], (minibatch_size,))\n",
    "\n",
    "    # ----- forward pass -----\n",
    "    # embedding and concatenating\n",
    "    emb = C[Xtr[minibatch]]\n",
    "    embcat = emb.view(-1, embedding_dim * context_length)\n",
    "    # Linear layer 1\n",
    "    h1preact = embcat @ W1 + b1\n",
    "    # Batch Norm\n",
    "    h1mean = h1preact.mean(0, keepdims=True)\n",
    "    h1var = h1preact.var(0, keepdims=True)\n",
    "    h1shifted = h1preact - h1mean\n",
    "    h1scalefactor = torch.sqrt(h1var + epsilon)\n",
    "    xhat = h1shifted / h1scalefactor\n",
    "    yhat = gamma * xhat + beta\n",
    "    # Activation\n",
    "    h1 = torch.tanh(yhat)\n",
    "    # Linear layer 2 \n",
    "    h2 = h1 @ W2 + b2\n",
    "    # Calculate the loss function\n",
    "    counts = h2.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    logprobs = -probs.log()\n",
    "    predprob = logprobs[torch.arange(minibatch_size), Ytr[minibatch]]\n",
    "    loss = predprob.mean()\n",
    "    \n",
    "    # calculate gradient\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update values\n",
    "    for p in parameters:\n",
    "        p.data -= lr * p.grad\n",
    "    \n",
    "    # print progress\n",
    "    if step % 2000 == 0:\n",
    "        print(f'{step}/{max_iterations} loss = {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have something that works as intended. This is good. Now we will just import the code that Andrej wrote so that we are in unision with the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
